\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:intro}{{1}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Goals}{1}{section.1.1}}
\citation{pattersonhennessy}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}GPU Architecture and Data Parallelism}{2}{subsection.1.1.1}}
\citation{pipemicrosoft}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Other Applications}{3}{subsection.1.1.2}}
\newlabel{subs:otherApps}{{1.1.2}{3}{Other Applications}{subsection.1.1.2}{}}
\citation{rendering}
\citation{cudapipe}
\citation{fromCUtoOCL}
\citation{backtrack}
\citation{backtrack}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}GP-GPUs and Stream Parallel}{6}{subsection.1.1.3}}
\newlabel{subs:gpgpustreampar}{{1.1.3}{6}{GP-GPUs and Stream Parallel}{subsection.1.1.3}{}}
\citation{spm}
\citation{parpattbench}
\citation{parpattbench}
\citation{parpattbench}
\citation{spm}
\citation{spm}
\citation{streamparpatt}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Expectations}{9}{section.1.2}}
\citation{cpugpumix}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Results}{10}{section.1.3}}
\citation{cudabestpractices}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Tools}{11}{section.1.4}}
\newlabel{sect:tools}{{1.4}{11}{Tools}{section.1.4}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Tools}{13}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:tools}{{2}{13}{Tools}{chapter.2}{}}
\citation{cudaguide}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}NVIDIA Architecture and CUDA}{14}{section.2.1}}
\newlabel{sect:nvidiaarch}{{2.1}{14}{NVIDIA Architecture and CUDA}{section.2.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces GPUs specifics for the two remote machines employed in this project.\relax }}{15}{table.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:gpuspecs}{{2.1}{15}{GPUs specifics for the two remote machines employed in this project.\relax }{table.caption.1}{}}
\citation{pattersonhennessy}
\citation{cudaguide}
\citation{cudaguide}
\citation{cudaguide}
\citation{perfoptimize}
\citation{understandlatency}
\citation{cudaguide}
\citation{cudaguide}
\citation{cudaguide}
\citation{cudahandbook}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces GPU scalability.\relax }}{19}{figure.caption.2}}
\newlabel{fig:cudaSM}{{2.1}{19}{GPU scalability.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Copy engines and Memory organization}{19}{subsection.2.1.1}}
\citation{cudahandbook}
\citation{custreamsblog}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Comparison between one and two copy engines, using default (above) and non-default CUDA streams (below).\relax }}{21}{figure.caption.3}}
\newlabel{fig:copyengines}{{2.2}{21}{Comparison between one and two copy engines, using default (above) and non-default CUDA streams (below).\relax }{figure.caption.3}{}}
\citation{cudahandbook}
\citation{cudabestpractices}
\citation{cudahandbook}
\citation{cudaguide}
\citation{cudaguide}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces CUDA memory hierarchy and SM organization.\relax }}{23}{figure.caption.4}}
\newlabel{fig:memhierarchy}{{2.3}{23}{CUDA memory hierarchy and SM organization.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}CUDA C/C++}{23}{section.2.2}}
\newlabel{sect:CUDAcpp}{{2.2}{23}{CUDA C/C++}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Kernels}{23}{subsection.2.2.1}}
\newlabel{subs:ker}{{2.2.1}{23}{Kernels}{subsection.2.2.1}{}}
\citation{cudaguide}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Grid/Block organization: above a Grid formed by Blocks; below a Block formed by Threads.\relax }}{24}{figure.caption.5}}
\newlabel{fig:gridblock}{{2.4}{24}{Grid/Block organization: above a Grid formed by Blocks; below a Block formed by Threads.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Thread Hierarchy}{24}{subsection.2.2.2}}
\newlabel{subs:thrhierarchy}{{2.2.2}{24}{Thread Hierarchy}{subsection.2.2.2}{}}
\citation{cudaguide}
\citation{cudahandbook}
\citation{cudaguide}
\citation{cudastrandconcurr}
\citation{cudaguide}
\citation{custreamsblog}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}CUDA Streams}{25}{subsection.2.2.3}}
\newlabel{subs:streams}{{2.2.3}{25}{CUDA Streams}{subsection.2.2.3}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2.1}CUDA Strams creation}{26}{lstlisting.2.1}}
\citation{nvccdoc}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2.2}CUDA Strams and Async example}{27}{lstlisting.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}\texttt  {nvcc} compiler}{27}{subsection.2.2.4}}
\citation{nvccdoc}
\citation{cudagdbdoc}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}\texttt  {cuda-gdb} debugger}{28}{subsection.2.2.5}}
\citation{cudagdbdoc}
\citation{profilersguide}
\citation{nvprofarticle}
\citation{profilersguide}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Profilers}{29}{section.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}\texttt  {nvprof}}{29}{subsection.2.3.1}}
\citation{profilersguide}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}NVIDIA Visual Profiler}{30}{subsection.2.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Visual Studio Code}{31}{section.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Tests, Result gathering, Plots}{31}{section.2.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Bash scripts}{32}{subsection.2.5.1}}
\newlabel{subs:bash}{{2.5.1}{32}{Bash scripts}{subsection.2.5.1}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2.3}Tests on bash scripts example}{32}{lstlisting.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Python scripts}{33}{subsection.2.5.2}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2.4}Portion of speedup and plots Python script}{33}{lstlisting.2.4}}
\citation{spm}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Project Logic}{37}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:logic}{{3}{37}{Project Logic}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Stream Parallelism: Farm pattern}{37}{section.3.1}}
\citation{spm}
\citation{parpattbench}
\citation{spm}
\citation{spm}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Farm performance model}{39}{subsection.3.1.1}}
\newlabel{subs:farmperfmodel}{{3.1.1}{39}{Farm performance model}{subsection.3.1.1}{}}
\citation{spm}
\citation{spm}
\citation{spm}
\citation{spm}
\newlabel{eq:Tseq}{{3.4}{42}{Farm performance model}{equation.3.1.4}{}}
\newlabel{eq:TcompFarm}{{3.5}{42}{Farm performance model}{equation.3.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}CPU-GPGPU: heterogeneous architecture}{43}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Overlapping: Data Transfer hiding}{44}{subsection.3.2.1}}
\citation{cudabestpractices}
\citation{cudaguide}
\citation{cudabestpractices}
\citation{cudastrandconcurr}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Ideal behavior for 3 CUDA Streams.\relax }}{45}{figure.caption.6}}
\newlabel{fig:threeStreams}{{3.1}{45}{Ideal behavior for 3 CUDA Streams.\relax }{figure.caption.6}{}}
\citation{cudaguide}
\citation{cudaguide}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Occupancy of GPU cores}{47}{subsection.3.2.2}}
\citation{cudaguide}
\citation{perfoptimize}
\citation{understandlatency}
\citation{cudaguide}
\citation{understandlatency}
\citation{cudaguide}
\citation{cudaguide}
\citation{loweroccupancy}
\citation{perfoptimize}
\citation{loweroccupancy}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Occupancy drawbacks}{50}{subsection.3.2.3}}
\citation{loweroccupancy}
\citation{cudaguide}
\citation{loweroccupancy}
\citation{understandlatency}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Overall Logic}{52}{section.3.3}}
\newlabel{sect:overallLogic}{{3.3}{52}{Overall Logic}{section.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces In this picture we show the schema of Farm on GPU, here we have only one task to/from the GPU.\relax }}{53}{figure.caption.7}}
\newlabel{fig:H2D}{{3.2}{53}{In this picture we show the schema of Farm on GPU, here we have only one task to/from the GPU.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Here we have an overall and broad graphical representation of our idea on how to fit a Farm parallel pattern on GPU architecture.\relax }}{55}{figure.caption.8}}
\newlabel{fig:overallLogic}{{3.3}{55}{Here we have an overall and broad graphical representation of our idea on how to fit a Farm parallel pattern on GPU architecture.\relax }{figure.caption.8}{}}
\citation{cudaguide}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Legenda about Figure \ref  {fig:overallLogic}.\relax }}{57}{figure.caption.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces In this picture we can see what exactly happens in a certain CUDA Stream. Light violet numbered labels show the order in which commands are issued by host to the GPU by a generic stream.\relax }}{59}{figure.caption.10}}
\newlabel{fig:singleStream}{{3.5}{59}{In this picture we can see what exactly happens in a certain CUDA Stream. Light violet numbered labels show the order in which commands are issued by host to the GPU by a generic stream.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Tunings}{59}{section.3.4}}
\newlabel{sect:tunings}{{3.4}{59}{Tunings}{section.3.4}{}}
\citation{cudabestpractices}
\citation{cudabestpractices}
\citation{cudaguide}
\citation{perfoptimize}
\citation{cudaguide}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Tuning on block and grid dimensions}{61}{subsection.3.4.1}}
\citation{cudabestpractices}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Implementation}{63}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:impl}{{4}{63}{Implementation}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Kernels}{63}{section.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Simple-computational kernel}{64}{subsection.4.1.1}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.1}Implementation for Simple-Computation Kernel}{64}{lstlisting.4.1}}
\citation{cudaguide}
\citation{matmul}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Matrix multiplication}{65}{subsection.4.1.2}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.2}Implementation for Matrix Multiplication Kernel, both non-square and square}{65}{lstlisting.4.2}}
\citation{coalesced}
\citation{cudabestpractices}
\citation{cudaguide}
\citation{matmul}
\citation{blurbox}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Blur Box filter}{67}{subsection.4.1.3}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.3}Implementation for Image processing Kernel (Blur Box Algorithm)}{67}{lstlisting.4.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Parallel Patterns implementation on GPU}{69}{section.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Stream Parallel on GPU}{69}{subsection.4.2.1}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.4}Simple-computational Kernel Launch configuration, i.e. Grid and Block dimensions setting}{70}{lstlisting.4.4}}
\newlabel{lst:noStr}{{4.5}{71}{Data transfer host/device and kernel call, synchronous version}{lstlisting.4.5}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.5}Data transfer host/device and kernel call, synchronous version}{71}{lstlisting.4.5}}
\newlabel{lst:str}{{4.6}{72}{Data transfer host/device and kernel call, CUDA Streams version}{lstlisting.4.6}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.6}Data transfer host/device and kernel call, CUDA Streams version}{72}{lstlisting.4.6}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.7}Host side pseudo-code: input stream \& kernel call function(scheduler).}{73}{lstlisting.4.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Data Parallel un GPU}{74}{subsection.4.2.2}}
\citation{cudaguide}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.8}Optimal Kernel caller for Simple-Computation kernel, uses Occupancy APIs to get best Block configuration}{75}{lstlisting.4.8}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Experiments}{77}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:experim}{{5}{77}{Experiments}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Expectations}{77}{section.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Measures: What and How}{78}{subsection.5.1.1}}
\newlabel{lst:timers}{{5.1}{79}{Example of code for events utilization as time probes}{lstlisting.5.1}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.1}Example of code for events utilization as time probes.}{79}{lstlisting.5.1}}
\citation{devblogevents}
\citation{cudaguide}
\citation{devblogevents}
\citation{cudaguide}
\citation{cudaguide}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Tests setup}{83}{subsection.5.1.2}}
\citation{spm}
\citation{structparprog}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Speedup}{84}{subsection.5.1.3}}
\newlabel{subs:speedup}{{5.1.3}{84}{Speedup}{subsection.5.1.3}{}}
\newlabel{eq:speedup}{{5.1}{84}{Speedup}{equation.5.1.1}{}}
\citation{structparprog}
\citation{spm}
\newlabel{eq:parserfract1}{{5.2}{85}{Speedup}{equation.5.1.2}{}}
\newlabel{eq:parserfract2}{{5.3}{85}{Speedup}{equation.5.1.3}{}}
\newlabel{eq:serparfract}{{5.5}{85}{Speedup}{equation.5.1.5}{}}
\newlabel{eq:amdahlupperbound}{{5.6}{85}{Speedup}{equation.5.1.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.4}Results: gathering and evaluation}{85}{subsection.5.1.4}}
\newlabel{subs:resgath}{{5.1.4}{85}{Results: gathering and evaluation}{subsection.5.1.4}{}}
\newlabel{eq:totTime}{{5.7}{87}{Results: gathering and evaluation}{equation.5.1.7}{}}
\citation{rooflinepaper}
\newlabel{eq:SMupperbound}{{5.9}{89}{Results: gathering and evaluation}{equation.5.1.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.5}Computation-bound and memory-bound}{89}{subsection.5.1.5}}
\newlabel{sect:roofline}{{5.1.5}{89}{Computation-bound and memory-bound}{subsection.5.1.5}{}}
\citation{rooflineslides}
\citation{optimizingcuda}
\citation{rooflineslides}
\citation{rooflinepaper}
\citation{rooflineslides}
\citation{rooflinepaper}
\citation{applyroofline}
\citation{rooflinepaper}
\citation{applyroofline}
\citation{p100whitepaper}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Simple-computation kernel}{93}{section.5.2}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Input dataset for Simple-Computation kernel, these are the input stream limits for both devices.\relax }}{94}{table.caption.11}}
\newlabel{tab:cosdata}{{5.1}{94}{Input dataset for Simple-Computation kernel, these are the input stream limits for both devices.\relax }{table.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Results}{97}{subsection.5.2.1}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Device completion times for Simple-computation kernel, without using CUDA Streams, results are reported for both machines (P100 and M40).\relax }}{98}{table.caption.12}}
\newlabel{tab:cosavgszero}{{5.2}{98}{Device completion times for Simple-computation kernel, without using CUDA Streams, results are reported for both machines (P100 and M40).\relax }{table.caption.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Device completion times for Simple-computation kernel, using as many CUDA Streams as SM number, results are reported for both machines (P100 and M40).\relax }}{99}{table.caption.13}}
\newlabel{tab:cosavgsSM}{{5.3}{99}{Device completion times for Simple-computation kernel, using as many CUDA Streams as SM number, results are reported for both machines (P100 and M40).\relax }{table.caption.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces Speedups for all data sets of simple-computation kernel. Results are reported for both devices.\relax }}{101}{table.caption.14}}
\newlabel{tab:cosspeedup}{{5.4}{101}{Speedups for all data sets of simple-computation kernel. Results are reported for both devices.\relax }{table.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Speedup for 3 and 56 CUDA streams on P100 device.\relax }}{102}{figure.caption.15}}
\newlabel{fig:p100sp}{{5.1}{102}{Speedup for 3 and 56 CUDA streams on P100 device.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Speedup for 3 and 56 CUDA streams on M40 device.\relax }}{102}{figure.caption.15}}
\newlabel{fig:m40sp}{{5.2}{102}{Speedup for 3 and 56 CUDA streams on M40 device.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Profiling for an example of execution: number of tasks 768, total amount of work load 786432 floats, kernel iterations 10 000, 24 CUDA streams, on M40 device.\relax }}{103}{figure.caption.16}}
\newlabel{fig:cosprofiling}{{5.3}{103}{Profiling for an example of execution: number of tasks 768, total amount of work load 786432 floats, kernel iterations 10 000, 24 CUDA streams, on M40 device.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Sublinear speedup in bigger buffers execution, the performances are clearly halved.\relax }}{105}{figure.caption.17}}
\newlabel{fig:biggerbufferspeedup}{{5.4}{105}{Sublinear speedup in bigger buffers execution, the performances are clearly halved.\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Matrix Multiplication}{105}{section.5.3}}
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces Input dataset for Matrix Multiplication kernel. Above Stream parallel configuration (task size and task amount), below the total work load for the overall executiont.\relax }}{106}{table.caption.18}}
\newlabel{tab:matdata}{{5.5}{106}{Input dataset for Matrix Multiplication kernel. Above Stream parallel configuration (task size and task amount), below the total work load for the overall executiont.\relax }{table.caption.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Results}{108}{subsection.5.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {5.6}{\ignorespaces Device completion times for Mat-Mul kernel, without using CUDA Streams (zero non-default streams), results are reported for both P100 and M40.\relax }}{109}{table.caption.19}}
\newlabel{tab:matvgszero}{{5.6}{109}{Device completion times for Mat-Mul kernel, without using CUDA Streams (zero non-default streams), results are reported for both P100 and M40.\relax }{table.caption.19}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.7}{\ignorespaces Device completion times for Mat-Mul kernel, with three CUDA Streams, results are reported for both P100 and M40.\relax }}{110}{table.caption.20}}
\newlabel{tab:matvgsThree}{{5.7}{110}{Device completion times for Mat-Mul kernel, with three CUDA Streams, results are reported for both P100 and M40.\relax }{table.caption.20}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.8}{\ignorespaces Device completion times for Mat-Mul kernel, with as many CUDA Streams as SM amount, results are reported for both P100 and M40.\relax }}{111}{table.caption.21}}
\newlabel{tab:matvgsSM}{{5.8}{111}{Device completion times for Mat-Mul kernel, with as many CUDA Streams as SM amount, results are reported for both P100 and M40.\relax }{table.caption.21}{}}
\citation{cudaguide}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Completion Time as the task size (matrix order) changes on M40.\relax }}{113}{figure.caption.22}}
\newlabel{fig:matcompsizeM40}{{5.5}{113}{Completion Time as the task size (matrix order) changes on M40.\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Completion Time as the task size (matrix order) changes on P100.\relax }}{113}{figure.caption.22}}
\newlabel{fig:matcompsizeP100}{{5.6}{113}{Completion Time as the task size (matrix order) changes on P100.\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Completion Time as the number of tasks (number of matrix multiplications) changes on M40.\relax }}{114}{figure.caption.23}}
\newlabel{fig:matcompnumM40}{{5.7}{114}{Completion Time as the number of tasks (number of matrix multiplications) changes on M40.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Completion Time as the number of tasks (number of matrix multiplications) changes on P100.\relax }}{114}{figure.caption.23}}
\newlabel{fig:matcompnumP100}{{5.8}{114}{Completion Time as the number of tasks (number of matrix multiplications) changes on P100.\relax }{figure.caption.23}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.9}{\ignorespaces Here are showed speedups for all data sets of matrix multiplication kernel. Results are reported for both devices.\relax }}{116}{table.caption.24}}
\newlabel{tab:matspeedup}{{5.9}{116}{Here are showed speedups for all data sets of matrix multiplication kernel. Results are reported for both devices.\relax }{table.caption.24}{}}
\newlabel{fig:timeln1024}{{5.9a}{117}{Subfigure 5 5.9a}{subfigure.5.9.1}{}}
\newlabel{sub@fig:timeln1024}{{(a)}{a}{Subfigure 5 5.9a\relax }{subfigure.5.9.1}{}}
\newlabel{fig:timeln256}{{5.9b}{117}{Subfigure 5 5.9b}{subfigure.5.9.2}{}}
\newlabel{sub@fig:timeln256}{{(b)}{b}{Subfigure 5 5.9b\relax }{subfigure.5.9.2}{}}
\newlabel{fig:timeln64}{{5.9c}{117}{Subfigure 5 5.9c}{subfigure.5.9.3}{}}
\newlabel{sub@fig:timeln64}{{(c)}{c}{Subfigure 5 5.9c\relax }{subfigure.5.9.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces NVIDIA Visual profiler generated \textit  {timeline} on M40, using 24 CUDA streams, running code for 784 tasks (matrix multiplications) and the different Task sizes (matrix orders) reported in captions above.\relax }}{117}{figure.caption.25}}
\newlabel{fig:timeln}{{5.9}{117}{NVIDIA Visual profiler generated \textit {timeline} on M40, using 24 CUDA streams, running code for 784 tasks (matrix multiplications) and the different Task sizes (matrix orders) reported in captions above.\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Task size 1024 }}}{117}{subfigure.9.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Task size 256 }}}{117}{subfigure.9.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Task size 64 }}}{117}{subfigure.9.3}}
\newlabel{fig:SMuse64}{{5.10a}{118}{Subfigure 5 5.10a}{subfigure.5.10.1}{}}
\newlabel{sub@fig:SMuse64}{{(a)}{a}{Subfigure 5 5.10a\relax }{subfigure.5.10.1}{}}
\newlabel{fig:SMuse1024}{{5.10b}{118}{Subfigure 5 5.10b}{subfigure.5.10.2}{}}
\newlabel{sub@fig:SMuse1024}{{(b)}{b}{Subfigure 5 5.10b\relax }{subfigure.5.10.2}{}}
\newlabel{fig:SMuse256}{{5.10c}{118}{Subfigure 5 5.10c}{subfigure.5.10.3}{}}
\newlabel{sub@fig:SMuse256}{{(c)}{c}{Subfigure 5 5.10c\relax }{subfigure.5.10.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces NVIDIA Visual profiler generated \textit  {timeline} on M40, using 24 CUDA streams, running code for 784 matrices and the different Task sizes (matrix orders) reported in captions above.\relax }}{118}{figure.caption.26}}
\newlabel{fig:SMuse}{{5.10}{118}{NVIDIA Visual profiler generated \textit {timeline} on M40, using 24 CUDA streams, running code for 784 matrices and the different Task sizes (matrix orders) reported in captions above.\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Task size 64}}}{118}{subfigure.10.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Task size 1024}}}{118}{subfigure.10.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Task size 256}}}{118}{subfigure.10.3}}
\citation{loweroccupancy}
\citation{cudabestpractices}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces Nsight profiler execution on M40, 24 CUDA streams, 784 matrices of size 256. This pie chart gives the types and amounts of latencies inside mat-mul kernel execution.\relax }}{120}{figure.caption.27}}
\newlabel{fig:mat62latency}{{5.11}{120}{Nsight profiler execution on M40, 24 CUDA streams, 784 matrices of size 256. This pie chart gives the types and amounts of latencies inside mat-mul kernel execution.\relax }{figure.caption.27}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Image processing}{121}{section.5.4}}
\@writefile{lot}{\contentsline {table}{\numberline {5.10}{\ignorespaces Input dataset for Image Processing kernel. Above Stream parallel configuration, below the relative Data Parallel.\relax }}{122}{table.caption.28}}
\newlabel{tab:imgdata}{{5.10}{122}{Input dataset for Image Processing kernel. Above Stream parallel configuration, below the relative Data Parallel.\relax }{table.caption.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Results}{123}{subsection.5.4.1}}
\@writefile{lot}{\contentsline {table}{\numberline {5.11}{\ignorespaces Device completion times for Image processing kernel, all types of tested CUDA Streams number are reported, results are given for both P100 and M40.\relax }}{124}{table.caption.29}}
\newlabel{tab:imgavgs}{{5.11}{124}{Device completion times for Image processing kernel, all types of tested CUDA Streams number are reported, results are given for both P100 and M40.\relax }{table.caption.29}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.12}{\ignorespaces Here we showed speedups for all data sets of image processing kernel. Results are reported for both devices.\relax }}{125}{table.caption.30}}
\newlabel{tab:imgspeedup}{{5.12}{125}{Here we showed speedups for all data sets of image processing kernel. Results are reported for both devices.\relax }{table.caption.30}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Results Summary}{125}{section.5.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Stream parallel compared to Data parallel}{126}{subsection.5.5.1}}
\@writefile{lot}{\contentsline {table}{\numberline {5.13}{\ignorespaces Here are showed completion time and speedup for \(48 = 2 \cdot \#SM\) CUDA Streams. \relax }}{127}{table.caption.31}}
\newlabel{tab:cosdoublestream}{{5.13}{127}{Here are showed completion time and speedup for \(48 = 2 \cdot \#SM\) CUDA Streams. \relax }{table.caption.31}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.14}{\ignorespaces Simple-computational kernel. Comparison between completion times for stream parallel (max stream -56 and 24 respectively-) and data parallel versions. Results are reported for both devices.\relax }}{129}{table.caption.32}}
\newlabel{tab:cosdataparVSsm}{{5.14}{129}{Simple-computational kernel. Comparison between completion times for stream parallel (max stream -56 and 24 respectively-) and data parallel versions. Results are reported for both devices.\relax }{table.caption.32}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.15}{\ignorespaces Matrix multiplication kernel. Comparison between completion times for stream parallel (max stream -56) and data parallel versions (Partial dataset of stream version is considered). Results are reported for P100.\relax }}{130}{table.caption.33}}
\newlabel{tab:matdataparVSsmP100}{{5.15}{130}{Matrix multiplication kernel. Comparison between completion times for stream parallel (max stream -56) and data parallel versions (Partial dataset of stream version is considered). Results are reported for P100.\relax }{table.caption.33}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.16}{\ignorespaces Matrix multiplication kernel. Comparison between completion times for stream parallel (max stream -24-) and data parallel versions (Partial dataset of stream version is considered). Results are reported for M40.\relax }}{131}{table.caption.34}}
\newlabel{tab:matdataparVSsmM40}{{5.16}{131}{Matrix multiplication kernel. Comparison between completion times for stream parallel (max stream -24-) and data parallel versions (Partial dataset of stream version is considered). Results are reported for M40.\relax }{table.caption.34}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.17}{\ignorespaces Here is showed the data parallel and stream parallel comparison for image processing kernel. Results are reported for both devices.\relax }}{132}{table.caption.35}}
\newlabel{tab:imgdataparVSsm}{{5.17}{132}{Here is showed the data parallel and stream parallel comparison for image processing kernel. Results are reported for both devices.\relax }{table.caption.35}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusions}{133}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:conclusions}{{6}{133}{Conclusions}{chapter.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Evaluation of the problem}{134}{subsection.6.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}Implementation and tests}{137}{subsection.6.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.4}Results and considerations}{138}{subsection.6.1.4}}
\citation{cudabestpractices}
\citation{cudaguide}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.5}Final remarks and further works}{140}{subsection.6.1.5}}
\citation{cudabestpractices}
\citation{loweroccupancy}
\citation{cudaguide}
\citation{cudaguide}
\citation{matmul}
\citation{loweroccupancy}
\citation{matmul}
\bibcite{pattersonhennessy}{1}
\bibcite{fromCUtoOCL}{2}
\bibcite{backtrack}{3}
\bibcite{pipemicrosoft}{4}
\bibcite{rendering}{5}
\bibcite{cudapipe}{6}
\bibcite{spm}{7}
\bibcite{cpugpumix}{8}
\bibcite{streamparpatt}{9}
\bibcite{cudaguide}{10}
\bibcite{profilersguide}{11}
\bibcite{nvprofarticle}{12}
\bibcite{loweroccupancy}{13}
\bibcite{understandlatency}{14}
\bibcite{devblogevents}{15}
\bibcite{libevents}{16}
\bibcite{structparprog}{17}
\bibcite{cudabestpractices}{18}
\bibcite{parpattbench}{19}
\bibcite{custreamsblog}{20}
\bibcite{nvccdoc}{21}
\bibcite{cudagdbdoc}{22}
\bibcite{coalesced}{23}
\bibcite{cudahandbook}{24}
\bibcite{blurbox}{25}
\bibcite{p100whitepaper}{26}
\bibcite{cudastrandconcurr}{27}
\bibcite{perfoptimize}{28}
\bibcite{rooflinepaper}{29}
\bibcite{rooflineslides}{30}
\bibcite{applyroofline}{31}
\bibcite{optimizingcuda}{32}
\bibcite{matmul}{33}
\@input{chaps/ringraziamenti.aux}
