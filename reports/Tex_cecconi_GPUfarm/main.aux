\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:intro}{{1}{1}{Introduction}{chapter.1}{}}
\citation{pattersonhennessy}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Goals}{2}{section.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}GPU Architecture and Data Parallel}{2}{subsection.1.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Other Applications}{2}{subsection.1.1.2}}
\newlabel{subs:otherApps}{{1.1.2}{2}{Other Applications}{subsection.1.1.2}{}}
\citation{fromCUtoOCL}
\citation{backtrack}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}GP-GPUs and Stream Parallel}{4}{subsection.1.1.3}}
\citation{spm}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Expectations}{5}{section.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Results}{6}{section.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Tools}{6}{section.1.4}}
\newlabel{sect:tools}{{1.4}{6}{Tools}{section.1.4}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Tools}{9}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:tools}{{2}{9}{Tools}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}CUDA}{10}{section.2.1}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces GPUs specifics for the two remote machines employed in this project.\relax }}{11}{table.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:gpuspecs}{{2.1}{11}{GPUs specifics for the two remote machines employed in this project.\relax }{table.caption.1}{}}
\citation{cudaguide}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces GPU scalability.\relax }}{12}{figure.caption.2}}
\newlabel{fig:cudaSM}{{2.1}{12}{GPU scalability.\relax }{figure.caption.2}{}}
\citation{profilersguide}
\citation{nvprofarticle}
\citation{profilersguide}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Profilers}{13}{section.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}nvprof}{13}{subsection.2.2.1}}
\citation{profilersguide}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}NVIDIA Visual Profiler}{14}{subsection.2.2.2}}
\citation{cudaguide}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}CUDA C/C++}{15}{section.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Above: a Grid formed by Blocks.  Below: a Block formed by Threads.\relax }}{15}{figure.caption.3}}
\newlabel{fig:gridblock}{{2.2}{15}{Above: a Grid formed by Blocks.\\ Below: a Block formed by Threads.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Kernels}{15}{subsection.2.3.1}}
\newlabel{subs:ker}{{2.3.1}{15}{Kernels}{subsection.2.3.1}{}}
\citation{cudaguide}
\citation{cudaguide}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Thread Hierarchy}{16}{subsection.2.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}CUDA Streams}{16}{subsection.2.3.3}}
\newlabel{subs:streams}{{2.3.3}{16}{CUDA Streams}{subsection.2.3.3}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2.1}CUDA Strams creation}{17}{lstlisting.2.1}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2.2}CUDA Strams and Async example}{17}{lstlisting.2.2}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2.3}CUDA Strams destroy}{18}{lstlisting.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}nvcc compiler}{18}{subsection.2.3.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}\texttt  {cuda-gdb} debugger}{19}{subsection.2.3.5}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Visual Studio Code}{20}{section.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Tests, Result gathering, Plots}{20}{section.2.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Bash scripts}{21}{subsection.2.5.1}}
\newlabel{subs:bash}{{2.5.1}{21}{Bash scripts}{subsection.2.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Python scripts}{21}{subsection.2.5.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Project Logic}{22}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:logic}{{3}{22}{Project Logic}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Streaming Parallelism: Farm pattern}{22}{section.3.1}}
\citation{spm}
\citation{spm}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}CPU-GPGPU: heterogeneous architecture}{24}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Overlapping: Data Transfer hiding}{25}{subsection.3.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Ideal behavior for 3 CUDA Streams.\relax }}{26}{figure.caption.4}}
\newlabel{fig:threeStreams}{{3.1}{26}{Ideal behavior for 3 CUDA Streams.\relax }{figure.caption.4}{}}
\citation{cudaguide}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Occupancy of GPU cores}{28}{subsection.3.2.2}}
\citation{cudaguide}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Occupancy drawbacks}{30}{subsection.3.2.3}}
\citation{loweroccupancy}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Overall Logic}{32}{section.3.3}}
\newlabel{sect:overallLogica}{{3.3}{32}{Overall Logic}{section.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Here we have a general and broad graphical representation of our idea on how to fit a Farm parallel pattern on GPU architecture.\relax }}{33}{figure.caption.5}}
\newlabel{fig:H2D}{{3.2}{33}{Here we have a general and broad graphical representation of our idea on how to fit a Farm parallel pattern on GPU architecture.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Here we have a general and broad graphical representation of our idea on how to fit a Farm parallel pattern on GPU architecture.\relax }}{35}{figure.caption.6}}
\newlabel{fig:overallLogic}{{3.3}{35}{Here we have a general and broad graphical representation of our idea on how to fit a Farm parallel pattern on GPU architecture.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Legenda about Figure \ref  {fig:overallLogic}.\relax }}{37}{figure.caption.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Here we can see what exactly happens in a certain CUDA Stream. Light violet numbered labels shows the order in which commands are issued by host to a certain stream.\relax }}{39}{figure.caption.8}}
\newlabel{fig:singleStream}{{3.5}{39}{Here we can see what exactly happens in a certain CUDA Stream. Light violet numbered labels shows the order in which commands are issued by host to a certain stream.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Tunings}{39}{section.3.4}}
\newlabel{sect:tunings}{{3.4}{39}{Tunings}{section.3.4}{}}
\citation{cudaguide}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Tuning on block and grid dimensions}{41}{subsection.3.4.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}CPU/GPU Scheduling}{42}{section.3.5}}
\newlabel{sect:cpugpuscheduling}{{3.5}{42}{CPU/GPU Scheduling}{section.3.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Implementation}{44}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:impl}{{4}{44}{Implementation}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Kernels}{44}{section.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Repeated cosine}{45}{subsection.4.1.1}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.1}Implementation for Simple-Computation Kernel}{45}{lstlisting.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Matrix multiplication}{45}{subsection.4.1.2}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.2}Implementation for Matrix Multiplication Kernel, both non-square and square}{46}{lstlisting.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Blur Box filter}{47}{subsection.4.1.3}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.3}Implementation for Image processing Kernel (Blur Box Algorithm)}{47}{lstlisting.4.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Parallel Patterns implementation on GPU}{48}{section.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Stream Parallel on GPU}{48}{subsection.4.2.1}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.4}Kernel Launch configuration, ie Grid and Block dimensions setting}{49}{lstlisting.4.4}}
\newlabel{lst:noStr}{{4.5}{49}{Data transfer host/device and kernel call, NO-CUDA Streams version}{lstlisting.4.5}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.5}Data transfer host/device and kernel call, NO-CUDA Streams version}{49}{lstlisting.4.5}}
\newlabel{lst:str}{{4.6}{50}{Data transfer host/device and kernel call, CUDA Streams version}{lstlisting.4.6}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.6}Data transfer host/device and kernel call, CUDA Streams version}{50}{lstlisting.4.6}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.7}Host side pseudo-code: input stream + kernel launcher function}{51}{lstlisting.4.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Data Parallel un GPU}{52}{subsection.4.2.2}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.8}Optimal Kernel launcher for Simple-Computation kernel, uses APIs to get best Block configuration}{53}{lstlisting.4.8}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}CPU and GPU Mix}{54}{section.4.3}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Experiments}{55}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:experim}{{5}{55}{Experiments}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Expectations}{55}{section.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Measures: What and How}{56}{subsection.5.1.1}}
\newlabel{lst:timers}{{5.1.1}{57}{}{lstlisting.5.-4}{}}
\citation{devblogevents}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Tests setup}{61}{subsection.5.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Speedup}{61}{subsection.5.1.3}}
\newlabel{subs:speedup}{{5.1.3}{61}{Speedup}{subsection.5.1.3}{}}
\citation{structparprog}
\citation{structparprog}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.4}Results: gathering and evaluation}{63}{subsection.5.1.4}}
\newlabel{subs:resgath}{{5.1.4}{63}{Results: gathering and evaluation}{subsection.5.1.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Simple-computation kernel}{67}{section.5.2}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Input dataset for Simple-Computation kernel, these are the input stream length for both devices.\relax }}{68}{table.caption.9}}
\newlabel{tab:cosdata}{{5.1}{68}{Input dataset for Simple-Computation kernel, these are the input stream length for both devices.\relax }{table.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Results}{71}{subsection.5.2.1}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Device completion times for Simple-computation kernel, without using CUDA Streams, results are reported for both machines (P100 and M40).\relax }}{72}{table.caption.10}}
\newlabel{tab:cosavgszero}{{5.2}{72}{Device completion times for Simple-computation kernel, without using CUDA Streams, results are reported for both machines (P100 and M40).\relax }{table.caption.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Device completion times for Simple-computation kernel, using as many CUDA Streams as SM number, results are reported for both machines (P100 and M40).\relax }}{73}{table.caption.11}}
\newlabel{tab:cosavgsSM}{{5.3}{73}{Device completion times for Simple-computation kernel, using as many CUDA Streams as SM number, results are reported for both machines (P100 and M40).\relax }{table.caption.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces Here are showed speedups for all data sets of simple-computation kernel. Results are reported for both devices.\relax }}{74}{table.caption.12}}
\newlabel{tab:cosspeedup}{{5.4}{74}{Here are showed speedups for all data sets of simple-computation kernel. Results are reported for both devices.\relax }{table.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Speedup for 3 and 56 CUDA streams on P100 device.\relax }}{76}{figure.caption.13}}
\newlabel{fig:p100sp}{{5.1}{76}{Speedup for 3 and 56 CUDA streams on P100 device.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Speedup for 3 and 56 CUDA streams on M40 device.\relax }}{76}{figure.caption.13}}
\newlabel{fig:m40sp}{{5.2}{76}{Speedup for 3 and 56 CUDA streams on M40 device.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Profiling for an example execution: limit for input stream 786432, kernel iterations 10 000, 24 CUDA streams, on M40 device.\relax }}{77}{figure.caption.14}}
\newlabel{fig:cosprofiling}{{5.3}{77}{Profiling for an example execution: limit for input stream 786432, kernel iterations 10 000, 24 CUDA streams, on M40 device.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Sublinear speedup in bigger buffers execution, the performances drop by a factor of 2.\relax }}{78}{figure.caption.15}}
\newlabel{fig:biggerbufferspeedup}{{5.4}{78}{Sublinear speedup in bigger buffers execution, the performances drop by a factor of 2.\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Matrix Multiplication}{78}{section.5.3}}
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces Input dataset for Matrix Multiplication kernel. Above Stream parallel configuration, below Data Parallel correspondent.\relax }}{80}{table.caption.16}}
\newlabel{tab:matdata}{{5.5}{80}{Input dataset for Matrix Multiplication kernel. Above Stream parallel configuration, below Data Parallel correspondent.\relax }{table.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Results}{82}{subsection.5.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {5.6}{\ignorespaces Device completion times for Mat-Mul kernel, without using CUDA Streams (zero streams), results are reported for both P100 and M40.\relax }}{83}{table.caption.17}}
\newlabel{tab:matvgszero}{{5.6}{83}{Device completion times for Mat-Mul kernel, without using CUDA Streams (zero streams), results are reported for both P100 and M40.\relax }{table.caption.17}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.7}{\ignorespaces Device completion times for Mat-Mul kernel, with three CUDA Streams, results are reported for both P100 and M40.\relax }}{84}{table.caption.18}}
\newlabel{tab:matvgsThree}{{5.7}{84}{Device completion times for Mat-Mul kernel, with three CUDA Streams, results are reported for both P100 and M40.\relax }{table.caption.18}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.8}{\ignorespaces Device completion times for Mat-Mul kernel, with as many CUDA Streams as SM number, results are reported for both P100 and M40.\relax }}{85}{table.caption.19}}
\newlabel{tab:matvgsSM}{{5.8}{85}{Device completion times for Mat-Mul kernel, with as many CUDA Streams as SM number, results are reported for both P100 and M40.\relax }{table.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Completion Time as the matrix order changes on M40.\relax }}{87}{figure.caption.20}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Completion Time as the matrix order changes on P100.\relax }}{87}{figure.caption.20}}
\newlabel{fig:matcompsize}{{5.6}{87}{Completion Time as the matrix order changes on P100.\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Completion Time as the number of matrices changes on M40.\relax }}{88}{figure.caption.21}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Completion Time as the number of matrices changes on P100.\relax }}{88}{figure.caption.21}}
\newlabel{fig:matcompnum}{{5.8}{88}{Completion Time as the number of matrices changes on P100.\relax }{figure.caption.21}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.9}{\ignorespaces Here are showed speedups for all data sets of matrix multiplication kernel. Results are reported for both devices.\relax }}{89}{table.caption.22}}
\newlabel{tab:matspeedup}{{5.9}{89}{Here are showed speedups for all data sets of matrix multiplication kernel. Results are reported for both devices.\relax }{table.caption.22}{}}
\newlabel{fig:timeln1024}{{5.9a}{91}{Subfigure 5 5.9a}{subfigure.5.9.1}{}}
\newlabel{sub@fig:timeln1024}{{(a)}{a}{Subfigure 5 5.9a\relax }{subfigure.5.9.1}{}}
\newlabel{fig:timeln256}{{5.9b}{91}{Subfigure 5 5.9b}{subfigure.5.9.2}{}}
\newlabel{sub@fig:timeln256}{{(b)}{b}{Subfigure 5 5.9b\relax }{subfigure.5.9.2}{}}
\newlabel{fig:timeln64}{{5.9c}{91}{Subfigure 5 5.9c}{subfigure.5.9.3}{}}
\newlabel{sub@fig:timeln64}{{(c)}{c}{Subfigure 5 5.9c\relax }{subfigure.5.9.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces NVIDIA Visual profiler generated \textit  {timeline} on M40, using 24 CUDA streams and running code for 784 matrices.\relax }}{91}{figure.caption.23}}
\newlabel{fig:timeln}{{5.9}{91}{NVIDIA Visual profiler generated \textit {timeline} on M40, using 24 CUDA streams and running code for 784 matrices.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Matrix size 1024}}}{91}{subfigure.9.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Matrix size 256}}}{91}{subfigure.9.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Matrix size 64}}}{91}{subfigure.9.3}}
\citation{loweroccupancy}
\citation{cudabestpractices}
\newlabel{fig:SMuse64}{{5.10a}{92}{Subfigure 5 5.10a}{subfigure.5.10.1}{}}
\newlabel{sub@fig:SMuse64}{{(a)}{a}{Subfigure 5 5.10a\relax }{subfigure.5.10.1}{}}
\newlabel{fig:SMuse1024}{{5.10b}{92}{Subfigure 5 5.10b}{subfigure.5.10.2}{}}
\newlabel{sub@fig:SMuse1024}{{(b)}{b}{Subfigure 5 5.10b\relax }{subfigure.5.10.2}{}}
\newlabel{fig:SMuse256}{{5.10c}{92}{Subfigure 5 5.10c}{subfigure.5.10.3}{}}
\newlabel{sub@fig:SMuse256}{{(c)}{c}{Subfigure 5 5.10c\relax }{subfigure.5.10.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces NVIDIA Visual profiler generated \textit  {timeline} on M40, using 24 CUDA streams and running code for 784 matrices.\relax }}{92}{figure.caption.24}}
\newlabel{fig:SMuse}{{5.10}{92}{NVIDIA Visual profiler generated \textit {timeline} on M40, using 24 CUDA streams and running code for 784 matrices.\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Matrix size 64}}}{92}{subfigure.10.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Matrix size 1024}}}{92}{subfigure.10.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Matrix size 256}}}{92}{subfigure.10.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces Nsight profiler execution on M40, 24 CUDA streams, 784 matrices of size 256. This graph gives the types and amounts of latencies inside mat-mul kernel execution.\relax }}{93}{figure.caption.25}}
\newlabel{fig:mat62latency}{{5.11}{93}{Nsight profiler execution on M40, 24 CUDA streams, 784 matrices of size 256. This graph gives the types and amounts of latencies inside mat-mul kernel execution.\relax }{figure.caption.25}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Image processing}{94}{section.5.4}}
\@writefile{lot}{\contentsline {table}{\numberline {5.10}{\ignorespaces Input dataset for Image Processing kernel. Above Stream parallel configuration, below the relative Data Parallel.\relax }}{95}{table.caption.26}}
\newlabel{tab:imgdata}{{5.10}{95}{Input dataset for Image Processing kernel. Above Stream parallel configuration, below the relative Data Parallel.\relax }{table.caption.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Results}{96}{subsection.5.4.1}}
\@writefile{lot}{\contentsline {table}{\numberline {5.11}{\ignorespaces Device completion times for Image processing kernel, all types of tested CUDA Streams number are reported, results are given for both P100 and M40.\relax }}{97}{table.caption.27}}
\newlabel{tab:imgavgs}{{5.11}{97}{Device completion times for Image processing kernel, all types of tested CUDA Streams number are reported, results are given for both P100 and M40.\relax }{table.caption.27}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.12}{\ignorespaces Here are showed speedups for all data sets of image processing kernel. Results are reported for both devices.\relax }}{98}{table.caption.28}}
\newlabel{tab:imgspeedup}{{5.12}{98}{Here are showed speedups for all data sets of image processing kernel. Results are reported for both devices.\relax }{table.caption.28}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Results Summary}{98}{section.5.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Stream parallel compared to Data parallel}{99}{subsection.5.5.1}}
\@writefile{lot}{\contentsline {table}{\numberline {5.13}{\ignorespaces Here are showed completion time and speedup for \(48 = 2 \cdot \#SM\) CUDA Streams. \relax }}{100}{table.caption.29}}
\newlabel{tab:cosdoublestream}{{5.13}{100}{Here are showed completion time and speedup for \(48 = 2 \cdot \#SM\) CUDA Streams. \relax }{table.caption.29}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.14}{\ignorespaces Simple-computational kernel. Comparison between completion times for stream parallel (max stream -56 and 24 respectively-) and data parallel versions. Results are reported for both devices.\relax }}{101}{table.caption.30}}
\newlabel{tab:cosdataparVSsm}{{5.14}{101}{Simple-computational kernel. Comparison between completion times for stream parallel (max stream -56 and 24 respectively-) and data parallel versions. Results are reported for both devices.\relax }{table.caption.30}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.15}{\ignorespaces Matrix multiplication kernel. Comparison between completion times for stream parallel (max stream -56) and data parallel versions (Partial dataset is of stream version is considered). Results are reported for P100.\relax }}{103}{table.caption.31}}
\newlabel{tab:matdataparVSsmP100}{{5.15}{103}{Matrix multiplication kernel. Comparison between completion times for stream parallel (max stream -56) and data parallel versions (Partial dataset is of stream version is considered). Results are reported for P100.\relax }{table.caption.31}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.16}{\ignorespaces Matrix multiplication kernel. Comparison between completion times for stream parallel (max stream -24-) and data parallel versions (Partial dataset is of stream version is considered). Results are reported for M40.\relax }}{104}{table.caption.32}}
\newlabel{tab:matdataparVSsmM40}{{5.16}{104}{Matrix multiplication kernel. Comparison between completion times for stream parallel (max stream -24-) and data parallel versions (Partial dataset is of stream version is considered). Results are reported for M40.\relax }{table.caption.32}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.17}{\ignorespaces Here is showed the data parallel vs. stream parallel comparison for image processing kernel. Results are reported for both devices.\relax }}{105}{table.caption.33}}
\newlabel{tab:imgdataparVSsm}{{5.17}{105}{Here is showed the data parallel vs. stream parallel comparison for image processing kernel. Results are reported for both devices.\relax }{table.caption.33}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusions}{106}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:conclusions}{{6}{106}{Conclusions}{chapter.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.0.2}Evaluation of the problem}{107}{subsection.6.0.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.0.3}Implementation and tests}{109}{subsection.6.0.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.0.4}Results and considerations}{110}{subsection.6.0.4}}
\citation{cudabestpractices}
\citation{loweroccupancy}
\citation{loweroccupancy}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.0.5}Final remarks and further works}{112}{subsection.6.0.5}}
\bibcite{pattersonhennessy}{1}
\bibcite{fromCUtoOCL}{2}
\bibcite{backtrack}{3}
\bibcite{spm}{4}
\bibcite{cudaguide}{5}
\bibcite{profilersguide}{6}
\bibcite{nvprofarticle}{7}
\bibcite{loweroccupancy}{8}
\bibcite{devblogevents}{9}
\bibcite{libevents}{10}
\bibcite{structparprog}{11}
\bibcite{cudabestpractices}{12}
